# -*- coding: utf-8 -*-
"""prml_project_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zbzuz6nkKruP5LonK_--XLrWRgdZmxhU
"""

import pickle
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import cv2

# Function to download CIFAR-10 dataset
def download_cifar10_dataset():
    import requests
    cifar10_url = "https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz"
    response = requests.get(cifar10_url, stream=True)
    with open("cifar-10-python.tar.gz", "wb") as f:
        f.write(response.content)
    import tarfile
    with tarfile.open("cifar-10-python.tar.gz", "r:gz") as tar:
        tar.extractall()
    print("CIFAR-10 dataset downloaded and extracted successfully.")

# Load CIFAR-10 dataset
def load_cifar10_dataset():
    try:
        with open('cifar-10-batches-py/data_batch_1', 'rb') as fo:
            dict = pickle.load(fo, encoding='bytes')
            X_train = dict[b'data']
            y_train = np.array(dict[b'labels'])
        with open('cifar-10-batches-py/test_batch', 'rb') as fo:
            dict = pickle.load(fo, encoding='bytes')
            X_test = dict[b'data']
            y_test = np.array(dict[b'labels'])
        return (X_train, y_train), (X_test, y_test)
    except FileNotFoundError:
        print("CIFAR-10 dataset not found. Downloading...")
        download_cifar10_dataset()
        return load_cifar10_dataset()

(X_train, y_train), (X_test, y_test) = load_cifar10_dataset()

# Function to extract SIFT features from an image
def extract_sift_features(image):
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    sift = cv2.SIFT_create()
    keypoints, descriptors = sift.detectAndCompute(gray, None)
    if descriptors is not None and descriptors.shape[0] >= 128:
        return descriptors.flatten()[:128]
    else:
        return np.zeros((128,), dtype=np.float32)

# Extract SIFT features for all images in the dataset and pad them to have a consistent length
def extract_features(images):
    features = []
    for image in images:
        sift_feature = extract_sift_features(image.reshape(32, 32, 3))
        features.append(sift_feature)
    return np.array(features)

X_train_features = extract_features(X_train)
X_test_features = extract_features(X_test)

# Reshape labels to 1D arrays
y_train = y_train.ravel()
y_test = y_test.ravel()

# Split dataset into training and testing sets
X_train, X_val, y_train, y_val = train_test_split(X_train_features, y_train, test_size=0.2, random_state=42)

# Define classifiers
classifiers = {
    'SVM': SVC(),
    'Random Forest': RandomForestClassifier(),
    'k-NN': KNeighborsClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

# Train and evaluate classifiers
for clf_name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)
    print(f"{clf_name} Accuracy:", accuracy)

# Evaluate on test set
best_classifier_name = max(classifiers, key=lambda x: accuracy_score(y_val, classifiers[x].predict(X_val)))
best_classifier = classifiers[best_classifier_name]
test_accuracy = accuracy_score(y_test, best_classifier.predict(X_test_features))
print(f"Best Classifier ({best_classifier_name}) Test Accuracy:", test_accuracy)
